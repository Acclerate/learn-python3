import os
import base64
import fitz  # PyMuPDF
import argparse
import glob
import time
import re
import io
from PIL import Image
from openai import OpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. [ä¿®æ”¹ç‚¹] ä»ç³»ç»Ÿç¯å¢ƒå˜é‡è·å– API Key
# ä½ çš„ç¯å¢ƒå˜é‡åå¿…é¡»æ˜¯: SiliconFlow_API_KEY
API_KEY = os.getenv("SiliconFlow_API_KEY")

# æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
if not API_KEY:
    print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ 'SiliconFlow_API_KEY'")
    print("ğŸ’¡ æç¤º: å¦‚æœä½ åˆšåˆšæ·»åŠ äº†ç¯å¢ƒå˜é‡ï¼Œè¯·å°è¯•é‡å¯ VS Code æˆ– ç»ˆç«¯çª—å£ã€‚")
    # å¦‚æœä½ ä¸æƒ³ç”¨ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒæŠŠä¸Šé¢ä¸¤è¡Œæ³¨é‡Šæ‰ï¼Œç›´æ¥è§£é™¤ä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š:
    # API_KEY = "sk-ä½ çš„å¯†é’¥å†™åœ¨è¿™é‡Œ"
    exit(1)

# 2. æ¨¡å‹é€‰æ‹©
# æ³¨æ„ï¼šQwen3-VL-235B-Thinking çš„å®é™… API åç§°å¯èƒ½éœ€è¦ç¡®è®¤
# å¦‚æœæŠ¥é”™æ‰¾ä¸åˆ°æ¨¡å‹ï¼Œè¯·å°è¯•å›é€€åˆ° "Qwen/Qwen2.5-VL-72B-Instruct"
MODEL_NAME = "Qwen/Qwen3-VL-235B-A22B-Thinking" 
BASE_URL = "https://api.siliconflow.cn/v1"

# 3. å¹¶å‘è®¾ç½® (L0ç”¨æˆ·å»ºè®®è®¾ä¸º1ï¼Œç¨³å®šç¬¬ä¸€)
MAX_WORKERS = 1 

# 4. åˆ†å—å¤§å° (æ¯ 10 é¡µä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢å´©æºƒ)
BATCH_SIZE = 10

# 5. åˆå¹¶å¤§å° (æ¯ 60 é¡µåˆå¹¶ä¸ºä¸€ä¸ªå¤§æ–‡ä»¶)
MERGE_CHUNK_SIZE = 60

# 6. åˆ‡å›¾å¾®è°ƒ (å‘å››å‘¨å¤–æ‰©åƒç´ )
PADDING_PIXELS = 15

# ===========================================

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

def clean_ad_content(text):
    """å¼ºåŠ›å»å¹¿å‘Šå‡½æ•°"""
    if not text: return ""
    ad_patterns = [
        r"å…³æ³¨å…¬ä¼—å·\s*ã€\s*è€ƒ\s*ç ”\s*å°\s*èˆŸ\s*ã€‘",
        r"å…è´¹è€ƒç ”èµ„æ–™\s*&\s*æ— æ°´å°\s*PDF",
        r"æ— \s*æ°´\s*å°\s*PDF",
        r"æ‰«\s*æ\s*äºŒ\s*ç»´\s*ç ",
    ]
    cleaned_text = text
    for pattern in ad_patterns:
        cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE)
    return cleaned_text.strip()

def get_pdf_page_data(doc, page_num, zoom=2.0):
    try:
        page = doc.load_page(page_num)
        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
        img_bytes = pix.tobytes("png")
        base64_str = base64.b64encode(img_bytes).decode("utf-8")
        full_b64 = f"data:image/png;base64,{base64_str}"
        pil_img = Image.open(io.BytesIO(img_bytes))
        return pil_img, full_b64
    except Exception as e:
        print(f"âŒ é¡µé¢è¯»å–å¤±è´¥: {e}")
        return None, None

def perform_local_crop(content, pil_img):
    pattern = r'<CUT_IMG>(\d+),(\d+),(\d+),(\d+)</CUT_IMG>'
    
    def replace_with_cropped_image(match):
        try:
            x1_n, y1_n, x2_n, y2_n = map(int, match.groups())
            w, h = pil_img.size
            
            x1 = int(x1_n / 1000 * w)
            y1 = int(y1_n / 1000 * h)
            x2 = int(x2_n / 1000 * w)
            y2 = int(y2_n / 1000 * h)
            
            x1 = max(0, x1 - PADDING_PIXELS)
            y1 = max(0, y1 - PADDING_PIXELS)
            x2 = min(w, x2 + PADDING_PIXELS)
            y2 = min(h, y2 + PADDING_PIXELS)
            
            if x2 <= x1 + 5 or y2 <= y1 + 5: return ""

            crop_img = pil_img.crop((x1, y1, x2, y2))
            buffered = io.BytesIO()
            crop_img.save(buffered, format="PNG")
            img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            return f"\n\n![Figure](data:image/png;base64,{img_b64})\n\n"
        except Exception:
            return ""

    return re.sub(pattern, replace_with_cropped_image, content)

def process_page_workflow(args):
    page_idx, pil_img, b64_img = args
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ•™ææ’ç‰ˆä¸“å®¶ã€‚æ™ºèƒ½åˆ¤æ–­è½¬æ–‡å­—æˆ–åˆ‡å›¾ã€‚ä¸¥ç¦è¾“å‡ºä»»ä½•å¹¿å‘Šã€‚"
    user_prompt = (
        "è¯·å¤„ç†è¿™å¼ å›¾ç‰‡ï¼š\n"
        "1. **æ–‡æœ¬/å…¬å¼** -> è½¬ Markdown/LaTeXã€‚\n"
        "2. **å¤æ‚å›¾/ç«–å¼** -> è¾“å‡ºåˆ‡å‰²æŒ‡ä»¤ `<CUT_IMG>xmin,ymin,xmax,ymax</CUT_IMG>` (0-1000åæ ‡)ï¼Œ**å•ç‹¬å ä¸€è¡Œ**ã€‚\n"
        "3. **è´Ÿé¢çº¦æŸ**ï¼šç»å¯¹ç¦æ­¢è¾“å‡ºäºŒç»´ç ã€â€œå…³æ³¨å…¬ä¼—å·â€ç­‰å¹¿å‘Šã€‚"
    )

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_prompt},
                            {"type": "image_url", "image_url": {"url": b64_img}},
                        ],
                    },
                ],
                temperature=0.1, max_tokens=4096
            )
            raw_content = response.choices[0].message.content
            final_content = perform_local_crop(raw_content, pil_img)
            final_content = clean_ad_content(final_content)
            return page_idx, final_content

        except Exception as e:
            if "429" in str(e):
                time.sleep(60)
            else:
                return page_idx, f""
    return page_idx, ""

def merge_markdowns_by_chunk(pdf_name, output_dir, final_dir, pages_per_file=60):
    """åˆ†å—åˆå¹¶å‡½æ•°"""
    print(f"ğŸ”„ æ­£åœ¨æŒ‰æ¯ {pages_per_file} é¡µåˆå¹¶æ–‡ä»¶...")
    pattern = os.path.join(output_dir, f"{pdf_name}_part_*.md")
    files = sorted(glob.glob(pattern))
    
    if not files: return

    def get_range(fname):
        match = re.search(r'_part_(\d+)_(\d+)', fname)
        return (int(match.group(1)), int(match.group(2))) if match else (0, 0)

    current_batch_files = []
    current_start_page = -1
    
    for i, fname in enumerate(files):
        s, e = get_range(fname)
        if current_start_page == -1: current_start_page = s
        
        current_batch_files.append(fname)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå¹¶
        current_pages = e - current_start_page + 1
        is_last = (i == len(files) - 1)
        
        if current_pages >= pages_per_file or is_last:
            merged_filename = f"{pdf_name}_Merged_{str(current_start_page).zfill(4)}_{str(e).zfill(4)}.md"
            merged_path = os.path.join(final_dir, merged_filename)
            
            with open(merged_path, "w", encoding="utf-8") as outfile:
                outfile.write(f"# {pdf_name} (Pages {current_start_page}-{e})\n\n")
                for batch_f in current_batch_files:
                    with open(batch_f, "r", encoding="utf-8") as infile:
                        outfile.write(infile.read())
                        outfile.write("\n\n---\n\n")
            
            print(f"ğŸ“¦ å·²ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {merged_filename}")
            current_batch_files = []
            current_start_page = -1
    print("ğŸ‰ æ‰€æœ‰åˆ†æ®µåˆå¹¶å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    if not os.path.exists(args.pdf_path):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return

    pdf_name = os.path.splitext(os.path.basename(args.pdf_path))[0]
    # ä¸´æ—¶å·¥ä½œç›®å½•
    work_dir = os.path.join(os.path.dirname(args.pdf_path), f"{pdf_name}_work_dir")
    os.makedirs(work_dir, exist_ok=True)
    
    doc = fitz.open(args.pdf_path)
    total_pages = doc.page_count
    
    print(f"ğŸš€ å¯åŠ¨å…¨è‡ªåŠ¨å¤„ç†: {pdf_name}")
    print(f"ğŸ“„ æ€»é¡µæ•°: {total_pages}")
    print(f"ğŸ“¦ åˆ†å—æ¨¡å¼: æ¯ {BATCH_SIZE} é¡µå­˜æ¡£ä¸€æ¬¡")
    print(f"â© æ–­ç‚¹ç»­ä¼ : å¼€å¯")
    print("-" * 40)

    # === [æ ¸å¿ƒé€»è¾‘] è‡ªåŠ¨å¾ªç¯åˆ†å—æ£€æŸ¥ ===
    for batch_start in range(0, total_pages, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, total_pages)
        
        # æ„é€ åˆ†å—æ–‡ä»¶å: book_part_0001_0010.md
        part_filename = f"{pdf_name}_part_{str(batch_start+1).zfill(4)}_{str(batch_end).zfill(4)}.md"
        part_path = os.path.join(work_dir, part_filename)

        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (æ–­ç‚¹ç»­ä¼ )
        if os.path.exists(part_path):
            print(f"âœ… [å·²å®Œæˆ] è·³è¿‡: ç¬¬ {batch_start+1}-{batch_end} é¡µ")
            continue
        
        # 2. å¦‚æœä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†
        print(f"âš¡ [æ­£åœ¨å¤„ç†] ç¬¬ {batch_start+1}-{batch_end} é¡µ ...")
        
        tasks = []
        for i in range(batch_start, batch_end):
            pil_img, b64_img = get_pdf_page_data(doc, i)
            if pil_img:
                tasks.append((i, pil_img, b64_img))

        batch_results = {}
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for res in tqdm(executor.map(process_page_workflow, tasks), total=len(tasks), unit="page", leave=False):
                batch_results[res[0]] = res[1]
                time.sleep(1) # ä¿æŠ¤ API

        # 3. ä¿å­˜åˆ†å—æ–‡ä»¶
        with open(part_path, "w", encoding="utf-8") as f:
            for i in range(batch_start, batch_end):
                if i in batch_results:
                    f.write(f"\n\n\n\n")
                    f.write(batch_results[i])
                    f.write("\n\n---\n\n")
        
        print(f"ğŸ’¾ å·²å­˜æ¡£: {part_filename}")
        time.sleep(2) # æ‰¹æ¬¡é—´ä¼‘æ¯

    doc.close()

    # === [æ ¸å¿ƒé€»è¾‘] æœ€ååˆ†å—åˆå¹¶ ===
    final_output_dir = os.path.dirname(args.pdf_path)
    merge_markdowns_by_chunk(pdf_name, work_dir, final_output_dir, pages_per_file=MERGE_CHUNK_SIZE)

if __name__ == "__main__":
    main()